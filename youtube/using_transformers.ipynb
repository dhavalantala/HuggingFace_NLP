{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "As you was in `chapter_1`, Tranformer models are usually very large. With millions to tens of *billions* of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task. \n",
    "\n",
    "The ðŸ¤— Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trainied, and saved. The library's main features are: \n",
    "\n",
    "* **Ease of use**: Downloading, loadingm and using a state-of-the-art NLP model for inference can be done in just two lines of code. \n",
    "* **Flexibility**: At their core, all models are simple PyTorch `nn.Module` or TrnsorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) framworks. \n",
    "* **Simplicity**: Hardly any abstractions are made across the library. The \"All in one file\" is a core concept: a model's forward pass is entirely defined in a single file, so that the code itself is unserstandable and hackable. \n",
    "\n",
    "This last feature makes ðŸ¤— Transformers quite different from other ML libraries. The models are not built on modules that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.\n",
    "\n",
    "This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in `Chapter 1`. Next, weâ€™ll discuss the model API: weâ€™ll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions.\n",
    "\n",
    "Then weâ€™ll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, weâ€™ll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behind the pipeline**\n",
    "\n",
    "Let's start with a complete, taking a look at what happened behind the scenes when we executed the following code in *Chapter 1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9972347617149353},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for you.\",\n",
    "        \"I hate this so much!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in *chapter 1*,  this pipeline groups together three steps: preprocessing, pasing the inputs through the model, and postprecessing:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "\n",
    "Let's quickly go over each of these.\n",
    "\n",
    "## **Preprocessing with a tokenizer**\n",
    "\n",
    "Like other neural networks, transformer models can't process raw text directly, so the first step of our pieline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:\n",
    "\n",
    "* Splitting the input words, subwords, or symbols (like punctuation) that are called *tokens*. \n",
    "* Mapping each token to an integer.\n",
    "* Adding additional inputs that may be useful to the model.\n",
    "\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the *`AutoTokenizer`* class and its *`from_pretrained()`* method. Using the checkpoint name of out model, it will automatically fetch the data associated with the model's tokenizer and cache it(so it's only downloaded the first time you run the code below).\n",
    "\n",
    "Since the default checkpoint of the *`sentiment-analysis`* pipeline is *`distilbert-base-uncased-finetuned-sst-2-english`* (you can see its models card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model! The only things left to do is to convert the list of input IDs to tensors. \n",
    "\n",
    "You can use ðŸ¤— Transformers without having to worry about which ML framework is used as a backbend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept *tensor* as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays insted. A NumPy array van be (OD), a vector (1D), a matrix(2D), or have more dimensions. It's effectively a tensor; other ML frameworks' tensors behave similarly, and are usually as simple to instantiate as NumPy arrays. \n",
    "\n",
    "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the `return_tensors` argument:\n",
    "\n",
    "``` python\n",
    "raw_input = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "input = tokenizer(raw_input, padding=True, truncation=True, return_tensors='pt')\n",
    "print(input)\n",
    "```\n",
    "Donâ€™t worry about padding and truncation just yet; weâ€™ll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
    "\n",
    "``` python\n",
    "output:> {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
    "                               [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,    0,     0,     0,     0,     0,     0]]), \n",
    "        'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                                  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
    "```\n",
    "\n",
    "The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. `input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_input = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_input, padding=True, truncation=True, return_tensors='pt')\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Going through the model**\n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. ðŸ¤— Transformers provides an `AutoModel` class which also has a `from_pretrained()` method:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "```\n",
    "\n",
    "In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it. \n",
    "\n",
    "This architecture contains only the base Transformer module; given some inputs, it outputs what we'll call *hidden states*, also known as *feature*. for each model input, we'll retrieve a high-dimensional vector representing the **contextual understanding of that input by the transformer model**.\n",
    "\n",
    "If this doesn't make sense, don't worry about it.\n",
    "\n",
    "While thses hidden states can be useful on their own, they're usually inputs to another part of the model, known as the *head*. The different taks could tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A high-dimensinal vector?**\n",
    "\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "* **Batch size**: The number of sequences processed at a time(2 in our example).\n",
    "* **Sequence length**: The length of the numerical representation of the sequence (16 in our example).\n",
    "* **Hidden size**: The voctor dimension of each model input. \n",
    "\n",
    "It is said to be \"high dimentional\" because of the last value. The hidden size can be very large (768 is coomen for smaller models, and in larger models this can reach 3072). \n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model: \n",
    "```python \n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape) \n",
    "```\n",
    "\n",
    "Note that the output of ðŸ¤— Transformers models behave like `namedtuple`s or dictionaries. You can access the elements by attributes (like we did) or by key (`outputs[\"last_hidden_state\"]`), or even by index if you know exactly where the things you are looking for is (`outputs[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model heads: Making sense out of numbers**\n",
    "\n",
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n",
    "\n",
    "The output of the Transformer model is sent directly to the model head to be processed. \n",
    "\n",
    "In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that representation of the sentences.\n",
    "\n",
    "There are many different architectures available in ðŸ¤— Transformers, with each one designed around tacking a specific task. \n",
    "Here is a non- exhaustive list:\n",
    "\n",
    "* Model (retrieve the hidden states)\n",
    "* ForCausalLM\n",
    "* ForMaskedLM\n",
    "* ForMultipleChoice\n",
    "* ForQuestionAnswering\n",
    "* ForSequenceClassification\n",
    "* ForTokenClassification\n",
    "* and others ðŸ¤—\n",
    "\n",
    "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we wonâ€™t actually use the `AutoModel` class, but `AutoModelForSequenceClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
